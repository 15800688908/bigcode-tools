#!/usr/bin/env python

import os
import os.path as path
import sys
import json
import subprocess
from urllib.parse import urlencode
from argparse import ArgumentParser
from concurrent.futures import ThreadPoolExecutor
import logging


SEARCH_URL = "https://api.github.com/search/repositories"
REPO_URL = "https://api.github.com/repos/{full_name}"
DEFAULT_HEADERS = {"Accept": "application/vnd.github.drax-preview+json"}


DEFAULT_LICENSES = ",".join([
    "MIT",
    "Apache-2.0",
    "MPL-2.0",
    "BSD-2-Clause",
    "BSD-3-Clause",
    "BSD-4-Clause",
    "MS-PL"
])
DEFAULT_SIZE = "1000..100000"
DEFAULT_STARS = ">=10"


class Project:
    JSON_KEYS = ["id", "full_name", "name", "html_url", "clone_url", "language",
                 "stargazers_count", "size", "fork", "created_at", "updated_at"]
    def __init__(self, json_repo):
        for key in self.JSON_KEYS:
            setattr(self, key, json_repo[key])
        self.license = json_repo.get("license")



def download_git_project(project, output_dir):
    subprocess.run(["git", "clone", "--depth", "1", project.clone_url, output_dir])


def download_project(project, output_base_dir):
    try:
        output_dir = path.join(output_base_dir, project.name)
        if path.isdir(output_dir):
            logging.info("%s already exists", project.name)
            return
        download_git_project(project, output_dir)
    except Exception as e:
        logging.warning("could not download %s: %s", project.name, e)


def download_projects(projects, output_dir):
    with ThreadPoolExecutor() as executor:
        executor.map(lambda p: download_project(p, output_dir), projects)


def load_projects_from_file(input_file):
    with open(input_file, "r") as f:
        return [Project(project) for project in json.load(f)]


def download_projects_command(args):
    projects = load_projects_from_file(args.input_file)
    download_projects(projects, args.output_dir)


def request_exception_handler(request, exception):
    logging.warning("failed to fetch %s: %s", request.url, exception)


def filter_projects_by_license(projects, headers, licenses):
    import grequests

    reqs = [grequests.get(REPO_URL.format(full_name=p.full_name), headers=headers)
            for p in projects]
    resps = grequests.map(reqs, exception_handler=request_exception_handler)

    filtered_projects = []
    for i, project in enumerate(projects):
        resp = resps[i]
        if not resp or resp.status_code != 200:
            logging.warning("ignoring %s because no info could be fetched", project.full_name)
            continue

        project_license = resp.json().get("license")
        if not project_license or not project_license.get("spdx_id"):
            continue
        license_id = project_license.get("spdx_id")
        if license_id in licenses:
            project.license = license_id
            filtered_projects.append(project)
    return filtered_projects


def run_search(args):
    # XXX: must be imported after gevent.monkey
    import requests

    q = create_search_query(args)
    query = {"per_page": 100, "q": q, "sort": args.sort}
    headers = DEFAULT_HEADERS.copy()
    licenses = args.licenses.split(",")
    if args.token:
        headers["Authorization"] = "token {0}".format(args.token)
    else:
        logging.warning("you did not provide a GitHub authentication token, " +
                        "you may run in a rate limit issue." +
                        "go to https://github.com/settings/tokens to generate a token")

    url = "{0}?{1}".format(SEARCH_URL, urlencode(query))

    fetched_repos = []
    while url and len(fetched_repos) < args.max_repos:
        logging.info("progress: %s/%s", len(fetched_repos), args.max_repos)

        logging.debug("querying GitHub API: %s", url)
        r = requests.get(url, headers=headers)
        if r.status_code != 200:
            logging.error("failed to fetch %s: %s", url, r.text)
            break
        projects = [Project(repo) for repo in r.json()["items"]]
        filtered_projects = filter_projects_by_license(projects, headers, licenses)
        for project in filtered_projects:
            if len(fetched_repos) < args.max_repos:
                fetched_repos.append(project)
        url = r.links.get("next", {}).get("url")

    return fetched_repos


def create_search_query(args):
    query = []
    if args.keyword:
        query.append(args.keyword)
        if not getattr(args, "in"):
            query.append("in:name")

    search_fields = ["user", "language", "stars", "fork", "in", "size"]
    for field in search_fields:
        value = getattr(args, field)
        if value:
            value = value if isinstance(value, str) else json.dumps(value)
            query.append("{0}:{1}".format(field, value))
    return " ".join(query)


def save_projects_list(projects, output_file):
    project_dicts = [vars(project) for project in projects]
    with open(output_file, "w") as f:
        json.dump(project_dicts, f)


def search_projects_command(args):
    # XXX: gevent seems to have issues with Python 3.6
    import gevent.monkey
    gevent.monkey.patch_ssl()

    projects = run_search(args)
    save_projects_list(projects, args.output)


def create_parser():
    parser = ArgumentParser(prog="apache-projects",
                            description="Fetch and download Apache projects")
    parser.add_argument("-v", action="count", help="Increase verbosity", default=0)
    subparsers = parser.add_subparsers(dest="command")

    search_projects_parser = subparsers.add_parser("search", help="search projects")
    search_projects_parser.add_argument("-o", "--output", default="repositories.json")
    search_projects_parser.add_argument("--language", help="language to search", required=True)
    search_projects_parser.add_argument("--keyword", help="keyword to search for")
    search_projects_parser.add_argument("--keyword-in", help="place to search for keyword",
                                        dest="in")
    search_projects_parser.add_argument("--include-forks", help="keyword to search for",
                                        default=False, action="store_true", dest="fork")
    search_projects_parser.add_argument("--user", help="limit search to user or org")
    search_projects_parser.add_argument(
        "--sort", help="results sort order, default: stars", default="stars")
    search_projects_parser.add_argument(
        "--size", help="size of the repository, default: {0}".format(DEFAULT_SIZE),
        default=DEFAULT_SIZE)
    search_projects_parser.add_argument(
        "--max-repos", help="total repos to find, default: 100", type=int, default=100)
    search_projects_parser.add_argument(
        "--stars", help="number of stars, default: {0}".format(DEFAULT_STARS),
        default=DEFAULT_STARS)
    search_projects_parser.add_argument(
        "--licenses", help="acceptable license, default: {0}".format(DEFAULT_LICENSES),
        default=DEFAULT_LICENSES)
    search_projects_parser.add_argument(
        "--token", default=os.environ.get("GITHUB_TOKEN"),
        help="GitHub authentication token. Can be passed with GITHUB_TOKEN env variable")

    download_projects_parser = subparsers.add_parser("download", help="download projects in list")
    download_projects_parser.add_argument(
        "-i", "--input-file", help="JSON file generated by search command")
    download_projects_parser.add_argument(
        "-o", "--output-dir", help="directory where to download projects")

    return parser


def run_command(args):
    if args.command == "search":
        search_projects_command(args)
    elif args.command == "download":
        download_projects_command(args)


def main(argv):
    parser = create_parser()
    args = parser.parse_args(argv)
    log_level = logging.WARN - (args.v * 10)
    logging.basicConfig(level=log_level)
    if not args.command:
        parser.error("no command provided")
    run_command(args)


if __name__ == "__main__":
    main(sys.argv[1:])
